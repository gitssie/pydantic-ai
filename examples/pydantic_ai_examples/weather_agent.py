"""Example of PydanticAI with multiple tools which the LLM needs to call in turn to answer a question.

In this case the idea is a "weather" agent â€” the user can ask for the weather in multiple cities,
the agent will use the `get_lat_lng` tool to get the latitude and longitude of the locations, then use
the `get_weather` tool to get the weather.

Run with:

    uv run -m pydantic_ai_examples.weather_agent
    
Note: Set the `GEMINI_API_KEY` environment variable to use the Gemini model.
"""

from __future__ import annotations as _annotations

import asyncio
import os
from dataclasses import dataclass
from typing import Any

import logfire
from httpx import AsyncClient, AsyncHTTPTransport

from pydantic_ai.agent import Agent
from pydantic_ai.messages import FunctionToolCallEvent, FunctionToolResultEvent, PartDeltaEvent, PartStartEvent, TextPart, TextPartDelta
from pydantic_ai.models.gemini import GeminiModel
from pydantic_ai.models.openai import OpenAIModel
from pydantic_ai.providers.google_gla import GoogleGLAProvider
from pydantic_ai.providers.openai import OpenAIProvider
from pydantic_ai_examples.code_agent import CodeAgent

# 'if-token-present' means nothing will be sent (and the example will work) if you don't have logfire configured
logfire.configure(send_to_logfire='if-token-present')


@dataclass
class Deps:
    client: AsyncClient
    weather_api_key: str | None
    geo_api_key: str | None



# ä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹åç§°
model_name = 'gemini-2.0-flash'
print(f'Using model: {model_name}')
# è®¾ç½®ä»£ç†
proxy = "http://192.168.31.119:3213"
# ä½¿ç”¨æ­£ç¡®çš„ Gemini æ¨¡å‹æ ¼å¼å’Œä»£ç†è®¾ç½®
transport = AsyncHTTPTransport(proxy=proxy)
custom_http_client = AsyncClient(transport=transport, timeout=30)
# è®¾ç½®Gemini APIå¯†é’¥
gemini_api_key = ""
gemini_model = GeminiModel( 
    model_name,
    provider=GoogleGLAProvider(api_key=gemini_api_key, http_client=custom_http_client),
)

gemini_model = OpenAIModel(
    model_name='gemini-2.0-flash',  
    provider=OpenAIProvider(base_url='https://generativelanguage.googleapis.com/v1beta/openai/',api_key=gemini_api_key, http_client=custom_http_client),  
)


weather_agent = CodeAgent[Deps, str](
    model=gemini_model,
    deps_type=Deps,
    output_type=str,
    retries=2,
    instrument=False,
)

@weather_agent.tool_plain
def get_lat_lng(location_description: str
) -> dict[str, float]:
    """Get the latitude and longitude of a location.

    Args:
        ctx: The context.
        location_description: A description of a location.
    """
    return {'lat': 39.9, 'lng': 116.4074} 




@weather_agent.tool_plain
def get_weather(lat: float, lng: float) -> dict[str, Any]:
    """Get the weather at a location.

    Args:
        ctx: The context.
        lat: Latitude of the location.
        lng: Longitude of the location.
    """
    return {'temperature': '21 Â°C', 'description': 'Sunny'}


async def main():
    async with AsyncClient() as client:
        # create a free API key at https://www.tomorrow.io/weather-api/
        weather_api_key = os.getenv('WEATHER_API_KEY')
        # create a free API key at https://geocode.maps.co/
        geo_api_key = os.getenv('GEO_API_KEY')
        deps = Deps(
            client=client, weather_api_key=weather_api_key, geo_api_key=geo_api_key
        )
        
        # print("ç¤ºä¾‹1: ä½¿ç”¨ stream_text() æµå¼è¾“å‡ºå®Œæ•´æ–‡æœ¬")
        # async with weather_agent.run_stream(
        #     'æŸ¥è¯¢æœ¬æœˆé”€å”®è®¢å•', deps=deps
        # ) as result:
        #     async for message in result.stream_text():
        #         print(f"æ”¶åˆ°: {message}")
        #
        #     # å®Œæˆåè·å–æœ€ç»ˆè¾“å‡º
        #     print("æœ€ç»ˆè¾“å‡º:", await result.get_output())
        
        # print("\nç¤ºä¾‹2: ä½¿ç”¨ stream_text(delta=True) æµå¼è¾“å‡ºå¢é‡æ–‡æœ¬")
        # async with weather_agent.run_stream(
        #     'æŸ¥è¯¢åŒ—äº¬å¤©æ°”', deps=deps
        # ) as result:
        #     async for message in result.stream():
        #         print(message,end="", flush=True)
            
        #     print("æœ€ç»ˆè¾“å‡º:", await result.get_output())

        print("\nç¤ºä¾‹2: ä½¿ç”¨ stream() æ–¹æ³•æµå¼è¾“å‡º")
        async with weather_agent.run_stream("ç°åœ¨æ—¥æœŸ", deps=deps) as result:
            async def stream_output():
                async for text in result.stream(debounce_by=0.01):
                    print(text, end="", flush=True)
            
            await stream_output()

        # æ·»åŠ æ–°æ¶ˆæ¯ï¼ˆä¾‹å¦‚ç”¨æˆ·æç¤ºå’Œä»£ç†å“åº”ï¼‰åˆ°æ•°æ®åº“
        print("\næ–°æ¶ˆæ¯JSON:", result.new_messages_json())

        print("\nç¤ºä¾‹3: ç›‘å¬å·¥å…·è°ƒç”¨å’Œæ‰§è¡Œè¿‡ç¨‹å¹¶æ”¯æŒå¢é‡æ–‡æœ¬è¾“å‡º")
        # é€šè¿‡ Agent.iter() æ–¹æ³•ç›´æ¥è·å–åº•å±‚çš„æ‰§è¡Œæµç¨‹
        async with weather_agent.iter(
            'ç°åœ¨æ—¥æœŸ', deps=deps
        ) as run:
            # ä½¿ç”¨ async for å¾ªç¯è‡ªåŠ¨å¤„ç†èŠ‚ç‚¹è¿­ä»£
            #current_text = ""
            
            async for node in run:
                if Agent.is_end_node(node):
                    if run.result is not None:
                        print(f"\nâœ… æœ€ç»ˆç»“æœ: {run.result.output}")
                    else:
                        print("\nâœ… æ‰§è¡Œå®Œæˆï¼Œä½†æ²¡æœ‰æœ€ç»ˆç»“æœ")
                    break
                    
                elif Agent.is_model_request_node(node):
                    async with node.stream(run.ctx) as request_stream:
                        async for event in request_stream:
                            if isinstance(event, PartDeltaEvent) and isinstance(event.delta, TextPartDelta):
                                # å¢é‡è¾“å‡ºæ–‡æœ¬
                                print(event.delta.content_delta, end="", flush=True)
                            elif isinstance(event, PartStartEvent) and isinstance(event.part, TextPart):
                                # è¾“å‡ºæ–°æ–‡æœ¬éƒ¨åˆ†çš„å¼€å§‹
                                print(event.part.content, end="", flush=True)
                                
                elif Agent.is_call_tools_node(node):
                    async with node.stream(run.ctx) as handle_stream:
                        async for event in handle_stream:
                            if isinstance(event, FunctionToolCallEvent):
                                print(f"âš™ï¸ è°ƒç”¨å·¥å…·: {event.part.tool_name}")
                                print(f"  å‚æ•°: {event.part.args_as_dict()}")
                                print(f"  è°ƒç”¨ID: {event.call_id}")
                            elif isinstance(event, FunctionToolResultEvent):
                                print(f"ğŸ“Š å·¥å…·ç»“æœ: {event.tool_call_id}")
                                print(f"  è¿”å›: {event.result.content}")
                else:
                    print(f"\nå…¶ä»–èŠ‚ç‚¹: {type(node).__name__}")


if __name__ == '__main__':
    asyncio.run(main())
