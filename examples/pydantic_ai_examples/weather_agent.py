"""Example of PydanticAI with multiple tools which the LLM needs to call in turn to answer a question.

In this case the idea is a "weather" agent â€” the user can ask for the weather in multiple cities,
the agent will use the `get_lat_lng` tool to get the latitude and longitude of the locations, then use
the `get_weather` tool to get the weather.

Run with:

    uv run -m pydantic_ai_examples.weather_agent
    
Note: Set the `GEMINI_API_KEY` environment variable to use the Gemini model.
"""

from __future__ import annotations as _annotations

import asyncio
import os
from dataclasses import dataclass
from typing import Annotated, Any, Union

from duckduckgo_search import DDGS
import logfire
from httpx import AsyncClient, AsyncHTTPTransport

import pydantic

from pydantic_ai import Agent
from pydantic_ai.common_tools.duckduckgo import duckduckgo_search_tool
from pydantic_ai.mcp import MCPServerStdio, MCPServerHTTP
from pydantic_ai.messages import ModelRequest, TextPart, ModelMessagesTypeAdapter, ModelResponse, FunctionToolCallEvent, \
    PartStartEvent, FunctionToolResultEvent, PartDeltaEvent, TextPartDelta
from pydantic_ai.models.gemini import GeminiModel
from pydantic_ai.models.openai import OpenAIModel
from pydantic_ai.providers.google_gla import GoogleGLAProvider
from pydantic_ai.providers.openai import OpenAIProvider
from pydantic_ai.tools import RunContext
from pydantic_ai_examples.code_agent import CodeAgent
from pydantic_ai_examples.xml_parser_model import XMLParsedModelResponse

# 'if-token-present' means nothing will be sent (and the example will work) if you don't have logfire configured
logfire.configure(send_to_logfire='if-token-present')


@dataclass
class Deps:
    client: AsyncClient
    weather_api_key: str | None
    geo_api_key: str | None



# ä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹åç§°
model_name = 'gemini-2.0-flash'
print(f'Using model: {model_name}')
# è®¾ç½®ä»£ç†
proxy = "http://172.16.20.19:3213"
# ä½¿ç”¨æ­£ç¡®çš„ Gemini æ¨¡å‹æ ¼å¼å’Œä»£ç†è®¾ç½®
transport = AsyncHTTPTransport(proxy=proxy)
custom_http_client = AsyncClient(transport=transport, timeout=30)
# è®¾ç½®Gemini APIå¯†é’¥
api_key = ""
model = GeminiModel( 
    model_name,
    provider=GoogleGLAProvider(api_key=api_key, http_client=custom_http_client),
)

model = OpenAIModel(
    model_name='gemini-2.0-flash',
    provider=OpenAIProvider(base_url='https://generativelanguage.googleapis.com/v1beta/openai/',api_key=api_key, http_client=custom_http_client),
)

model = OpenAIModel(
    model_name='deepseek-chat',
    provider=OpenAIProvider(base_url='https://api.deepseek.com/v1',api_key=api_key),
)

# åˆ›å»ºPlaywright MCPæœåŠ¡å™¨
# playwright_mcp_server = MCPServerStdio(
#     command="deno",
#     args=[
#         "run",
#         "-A",  # å…è®¸æ‰€æœ‰æƒé™ï¼Œç”Ÿäº§ç¯å¢ƒå¯ä»¥é™åˆ¶æ›´å¤š
#         "--unstable",
#         "npm:@playwright/mcp@latest",  # ä½¿ç”¨npmåŒ…
#         "--headless",  # æ— å¤´æ¨¡å¼è¿è¡Œæµè§ˆå™¨
#     ],
#     env=None,  # é»˜è®¤ç¯å¢ƒå˜é‡
# )
playwright_mcp_server = MCPServerHTTP(url='http://localhost:3000/sse')
python_mcp_server = MCPServerHTTP(url='http://localhost:3001/sse')

weather_agent = CodeAgent[Deps, str](
    model=model,
    additional_authorized_imports=['requests'],
    deps_type=Deps,
    output_type=str,
    retries=2,
    instrument=False,
    mcp_servers=[python_mcp_server],  # æ·»åŠ MCPæœåŠ¡å™¨
    tools = [duckduckgo_search_tool(DDGS(proxy=proxy), max_results=10)]
)

@weather_agent.tool_plain
def get_lat_lng(location_description: str
) -> dict[str, float]:
    """Get the latitude and longitude of a location.

    Args:
        ctx: The context.
        location_description: A description of a location.
    """
    return {'lat': 39.9, 'lng': 116.4074} 




@weather_agent.tool
def get_weather(ctx: RunContext[Deps], lat: float, lng: float) -> dict[str, Any]:
    """Get the weather at a location.

    Args:
        ctx: The context.
        lat: Latitude of the location.
        lng: Longitude of the location.
    """
    return {'temperature': '21 Â°C', 'description': 'Sunny'}


async def main():
    async with AsyncClient() as client:
        # create a free API key at https://www.tomorrow.io/weather-api/
        weather_api_key = os.getenv('WEATHER_API_KEY')
        # create a free API key at https://geocode.maps.co/
        geo_api_key = os.getenv('GEO_API_KEY')
        deps = Deps(
            client=client, weather_api_key=weather_api_key, geo_api_key=geo_api_key
        )
        
        # print("ç¤ºä¾‹1: ä½¿ç”¨ stream_text() æµå¼è¾“å‡ºå®Œæ•´æ–‡æœ¬")
        # async with weather_agent.run_stream(
        #     'æŸ¥è¯¢æœ¬æœˆé”€å”®è®¢å•', deps=deps
        # ) as result:
        #     async for message in result.stream_text():
        #         print(f"æ”¶åˆ°: {message}")
        #
        #     # å®Œæˆåè·å–æœ€ç»ˆè¾“å‡º
        #     print("æœ€ç»ˆè¾“å‡º:", await result.get_output())
        
        # print("\nç¤ºä¾‹2: ä½¿ç”¨ stream_text(delta=True) æµå¼è¾“å‡ºå¢é‡æ–‡æœ¬")
        # async with weather_agent.run_stream(
        #     'æŸ¥è¯¢åŒ—äº¬å¤©æ°”', deps=deps
        # ) as result:
        #     async for message in result.stream():
        #         print(message,end="", flush=True)
        #     print("æœ€ç»ˆè¾“å‡º:", await result.get_output())

  
        # node = ModelResponse(parts=[TextPart(content="Hello")])
        #
        # xml_node = XMLParsedModelResponse(parts=[TextPart(content="Hello")], raw_parts=[TextPart(content="Hello")])
        #
        # raw_parts=[TextPart(content="The Raw Parts Content")]
        #
        # # ä½¿ç”¨åå°„æœºåˆ¶æ·»åŠ  raw_parts å±æ€§
        # setattr(node, 'raw_parts', raw_parts)
        #
        # print(isinstance(node, ModelResponse))
        #
        # setattr(ModelMessagesTypeAdapter, '_type', list[ Annotated[Union[ModelRequest, ModelResponse,XMLParsedModelResponse], pydantic.Discriminator('kind')]])

        # response = ModelMessagesTypeAdapter.core_schema['schema']['items_schema']['choices']['response']
        # schema = response['schema']
      
        # response['config']['extra']= 'allow'
        # # å°†æ–°å­—æ®µæ·»åŠ åˆ°æ¨¡å‹çš„å­—æ®µåˆ—è¡¨ä¸­
        # schema['fields'].clear()
        # response['fields'].clear()
        #response['fields'].append('raw_parts')

        # åˆ›å»ºä¸€ä¸ªåŒ…å« raw_parts çš„ ModelMessage å®ä¾‹
        #del ModelMessagesTypeAdapter.core_schema
        #del ModelMessagesTypeAdapter.validator
        #print(typeAdapter.dump_json([node]))
        # row = ModelMessagesTypeAdapter.dump_json( [node,xml_node])
        # print(row)
        # node = ModelMessagesTypeAdapter.validate_json(row)
        # print(node)
        # print("\nç¤ºä¾‹2: ä½¿ç”¨ stream() æ–¹æ³•æµå¼è¾“å‡º")
        # async with weather_agent.run_stream("æŸ¥è¯¢åŒ—äº¬å¤©æ°”", deps=deps) as result:
        #     async def stream_output():
        #         async for text in result.stream(debounce_by=0.01):
        #             print(text, end="", flush=True)
        #
        #     await stream_output()
        #
        # # æ·»åŠ æ–°æ¶ˆæ¯ï¼ˆä¾‹å¦‚ç”¨æˆ·æç¤ºå’Œä»£ç†å“åº”ï¼‰åˆ°æ•°æ®åº“
        # try:
        #     messages = result.new_messages_json()
        #     print("\næ–°æ¶ˆæ¯JSON:", str(messages))
        # except Exception as e:
        #     print(f"é”™è¯¯: {e}")
        
        print("\nç¤ºä¾‹3: ç›‘å¬å·¥å…·è°ƒç”¨å’Œæ‰§è¡Œè¿‡ç¨‹å¹¶æ”¯æŒå¢é‡æ–‡æœ¬è¾“å‡º")

        async with weather_agent.run_mcp_servers():
            async with weather_agent.iter(
                '1. æˆ‘è¦æŸ¥è¯¢åŒ—äº¬å¤©æ°”\n2.å‘Šè¯‰æˆ‘3å¤©ä¹‹åæ˜¯ä»€ä¹ˆæ—¥æœŸï¼Ÿ\n3. å†™ä¸€ä¸ªforå¾ªç¯ç»™æˆ‘æµ‹è¯•ä¸€ä¸‹', deps=deps
            ) as run:
                # ä½¿ç”¨ async for å¾ªç¯è‡ªåŠ¨å¤„ç†èŠ‚ç‚¹è¿­ä»£
                #current_text = ""

                async for node in run:
                    if Agent.is_end_node(node):
                        if run.result is not None:
                            print(f"\nâœ… æœ€ç»ˆç»“æœ: {run.result.output}")
                        else:
                            print("\nâœ… æ‰§è¡Œå®Œæˆï¼Œä½†æ²¡æœ‰æœ€ç»ˆç»“æœ")
                        break

                    elif Agent.is_model_request_node(node):
                        async with node.stream(run.ctx) as request_stream:
                            async for event in request_stream:
                                if isinstance(event, PartDeltaEvent) and isinstance(event.delta, TextPartDelta):
                                    # å¢é‡è¾“å‡ºæ–‡æœ¬
                                    print(event.delta.content_delta, end="", flush=True)
                                elif isinstance(event, PartStartEvent) and isinstance(event.part, TextPart):
                                    # è¾“å‡ºæ–°æ–‡æœ¬éƒ¨åˆ†çš„å¼€å§‹
                                    print(event.part.content, end="", flush=True)

                    elif Agent.is_call_tools_node(node):
                        print("")
                        async with node.stream(run.ctx) as handle_stream:
                            async for event in handle_stream:
                                if isinstance(event, FunctionToolCallEvent):
                                    print(f"âš™ï¸ è°ƒç”¨å·¥å…·: {event.part.tool_name} ({event.call_id})")
                                    print(f"  å‚æ•°: {event.part.args_as_dict()}")
                                elif isinstance(event, FunctionToolResultEvent):
                                    print(f"ğŸ“Š å·¥å…·ç»“æœ: {event.result.content}")
                    else:
                        print(f"\nå…¶ä»–èŠ‚ç‚¹: {type(node).__name__}")


if __name__ == '__main__':
    asyncio.run(main())
